{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Excs(61-70).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN1lJenW1rnGXVLgaCgpygn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hari2301/Udemy_100-_Exercises-Python-DataScience/blob/main/Excs(61_70).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26SyCbWuXgH6"
      },
      "source": [
        "**Using the <em>KMeans</em> class (set <code>random_state=42</code>) from the <em>scikit-learn</em>, create a list of <em>WCSS (Within-Cluster Sum-of-Squared)</em> values for the number of clusters from 2 to 9 inclusive. Round WCSS values to two decimal places and print to the console.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpin1HcRUXee"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "df=pd.read_csv('clusters.csv')\n",
        "\n",
        "wcss=[]\n",
        "for i in range(2,10):\n",
        "    kmeans=KMeans(random_state=42,n_clusters=i)\n",
        "    kmeans.fit(df)\n",
        "    wcss.append(round(kmeans.inertia_,2))\n",
        "\n",
        "print(wcss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtGgulu-a9pw"
      },
      "source": [
        "**Using the <em>AgglomerativeClustering</em> class from the <em>scikit-learn</em>, create a model to split given dataset into two clusters. Make a prediction based on this model and assign a new column <code>'cluster'</code> which stores the cluster number for each sample in the<em> df DataFrame.</em>**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4APn-xua-XX"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "df=pd.read_csv('clusters.csv')\n",
        "\n",
        "agglo=AgglomerativeClustering(n_clusters=2)\n",
        "agglo.fit(df)\n",
        "\n",
        "df['cluster']=agglo.labels_\n",
        "print(df.head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcK-H8X-d46u"
      },
      "source": [
        "**Using the <em>AgglomerativeClustering</em> class from the <em>scikit-learn</em>, create a model to split given dataset into two clusters (use the <em>Manhattan </em>metric). Make a prediction based on this model and assign a new column <code>'cluster'</code> which stores the cluster number for each sample in the<em> df DataFrame</em>.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEh8lkLdbNwg"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "df=pd.read_csv('clusters.csv')\n",
        "\n",
        "agglo=AgglomerativeClustering(n_clusters=2,affinity='manhattan',linkage='complete')\n",
        "agglo.fit_predict(df)\n",
        "\n",
        "df['cluster']=agglo.labels_\n",
        "print(df.head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEmjZZX8d8IA"
      },
      "source": [
        "**Using the <em>DBSCAN</em> class from the <em>scikit-learn</em>, create a model to split given dataset into clusters. Set the following arguemnts:**\n",
        "\n",
        "<li>eps:0.6</li>\n",
        "<li>min_samples:7</li>\n",
        "\n",
        "**Make a prediction based on this model and assign a new column <code>'cluster'</code> which stores the cluster number for each sample in the <em>df DataFrame.</em> **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMeufquefbhv"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "df=pd.read_csv('clusters.csv')\n",
        "db=DBSCAN(eps=0.6,min_samples=7).fit(df)\n",
        "\n",
        "df['cluster'] = db.labels_\n",
        "print(df.head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_vJ7szehtKX"
      },
      "source": [
        "**Using the <em>DBSCAN</em> class from the <em>scikit-learn</em>, create a model to split given dataset into clusters. **\n",
        "\n",
        "**Make a prediction based on this model and assign a new column <code>'cluster'</code> which stores the cluster number for each sample in the <em>df DataFrame.</em>**\n",
        "\n",
        "**In response, print the distribution of the samples in each cluster.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxRr4Xl1gGpu"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "df=pd.read_csv('clusters.csv')\n",
        "\n",
        "db=DBSCAN(eps=0.6,min_samples=7).fit(df)\n",
        "df['cluster'] = db.labels_\n",
        "\n",
        "print(df.cluster.value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SmCqRHblF1H"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRW5KHw-lFDt"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "df = pd.read_csv('pca.csv')\n",
        "\n",
        "X = df.drop(columns=['class'])\n",
        "y = df['class']\n",
        "\n",
        "sc = StandardScaler()\n",
        "X_std=sc.fit_transform(X)\n",
        "\n",
        "print(X_std[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy8pGmw6rF8C"
      },
      "source": [
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "     \n",
        "     \n",
        "    np.set_printoptions(precision=8, suppress=True, edgeitems=5, linewidth=200)\n",
        "    np.random.seed(42)\n",
        "    df = pd.read_csv('pca.csv')\n",
        "     \n",
        "    X = df.copy()\n",
        "    y = X.pop('class')\n",
        "     \n",
        "    scaler = StandardScaler()\n",
        "    X_std = scaler.fit_transform(X)\n",
        "     \n",
        "    eig_vals, eig_vecs = np.linalg.eig(np.cov(X_std, rowvar=False))\n",
        "    eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:, i]) for i in range(len(eig_vals))]\n",
        "    eig_pairs.sort(reverse=True)\n",
        "     \n",
        "    W = np.hstack((eig_pairs[0][1].reshape(3, 1), eig_pairs[1][1].reshape(3, 1)))\n",
        "    X_pca = X_std.dot(W)\n",
        "    print(X_pca[:10])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}